<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <title>Zheng Zhan's Homepage</title>

  <meta name="author" content="Zheng Zhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="js/jquery.min.js"></script>
  <script src="js/jquery.scrollzer.min.js"></script>
  <script src="js/jquery.scrolly.min.js"></script>
  <script src="js/skel.min.js"></script>
  <script src="js/skel-layers.min.js"></script>

  <script type="text/javascript">
    function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
        e.style.display = 'inline';
      else
        e.style.display = 'none';
    }
  </script>

  <!-- <script src="js/init.js"></script> -->
  <script src="js/carousel.js"></script>

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
<!--   <link rel="icon" type="image/png" href="images/xx.jpg"> -->


  <style type="text/css">
    .carousel {
      -webkit-transform: translate3d(0, 0, 0);
      background: rgba(0, 0, 0, 0.85);
      position: fixed;
      right: 0;
      bottom: 0;
      min-width: 100%;
      min-height: 100%;
      width: auto;
      height: auto;
      display: none;
      z-index: 1;
    }

    .img-center-carousel {
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      padding: 0;
      margin: auto;
      width: 60%;
      height: auto;
    }

    .carousel-close {
      position: absolute;
      top: 25px;
      right: 100px;
      padding: 0;
      margin: auto;
      width: 50px;
      height: auto;
    }

    .research {
      margin-bottom: 30px;
    }

    .research h4 {
      float: left;
    }

    .research div {
      text-align: end;
      font-size: 0.9em;
    }

    .thumbnail {
      width: 100%;
      float: left;

    }

    .thumbnail-right {
      margin-left: 35%;
    }

    #exp li {
      margin-bottom: 50px;
    }

    .school-logo {
      width: 6%;
      float: left;
    }

    .school-text {
      margin-left: 10%;
      width: 60%;
    }
  </style>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zheng Zhan</name>
                  </p>
                  <p> I am a fourth-year PhD student in Machine Learning at <a href="https://www.northeastern.edu/">Northeastern University</a>, under the supervision of
                    Prof. <a href="https://web.northeastern.edu/yanzhiwang/#_ga=2.59245165.1964588443.1663640196-2055581220.1641240155">Yanzhi Wang</a>.
<!--                     I also work closely with Prof. <a href="https://ece.northeastern.edu/fac-ece/ioannidis/">Stratis
                      Ioannidis</a>,
                    and Prof. <a
                      href="https://web.northeastern.edu/yanzhiwang/#_ga=2.65760046.1809272488.1657261940-1545783568.1564022696">Yanzhi
                      Wang</a>.
                    I also work as student researcher at <a href="https://research.google/teams/cloud-ai/">Google Cloud
                      AI Research</a>,
                    under the supervision of <a href="https://sites.google.com/view/zizhaozhang/home">Zizhao Zhang</a>,
                    <a href="https://chl260.github.io/">Chen-Yu Lee</a>, and <a href="https://tomas.pfister.fi/">Tomas
                      Pfister</a>.
                    My other collaborators at Google includes <a href="https://people.eecs.berkeley.edu/~sayna/">Sayna
                      Ebrahimi</a>,
                    <a href="https://sites.google.com/view/hanzhang">Han Zhang</a>,
                    <a href="https://research.google/people/VincentPerot/">Vincent Perot</a>,
                    <a href="https://research.google/people/107194/">Ruoxi Sun</a>,
                    <a href="https://research.google/people/GuolongSu/">Guolong Su</a> and
                    <a href="https://resnick.caltech.edu/people/xiaoqi-ren">Xiaoqi Ren</a>. -->

                  </p>

                  <mark>
                    I am currently looking for full-time opportunities starting in 2023.
                  </mark>
                  </p>
                  
                  <p style="text-align:center">
                    <a href="mailto:zhan.zhe@northeastern.edu">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=hwTuEX0AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/zheng-zhan-1ba889150/">LinkedIn</a> &nbsp/&nbsp
                    <a href="Zheng_s_Resume.pdf">Resume</a>
                  </p>
                </td>
<!--                 <td style="padding:2.5%;width:20%;max-width:20%">
                  <a href="images/zifeng_avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zifeng_avatar.jpg" class="hoverZoomLink"></a>
                </td> -->
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                  <ul>
                    <li> Model compression </li>
                    <li> Continual (Lifelong) learning </li>
                    <li> Single Image Super-resolution </li>
                    <li> Deep learning applications on resource-limited platforms/scenarios. </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <ul>
                    <li> 09/2022: Our paper on efficient continual learning is accepted at <b> NeurIPS 2022 </b>  </li>
                    <li> 07/2022: Our paper All-in-One is accepted at <b> ICCAD 2022 </b>  </li>
                    <li> 07/2022: Our paper on compiler-aware NAS for real-time super-resolution on mobile is accepted at <b> ECCV 2022 </b>  </li>
                    <li> 11/2021: Our paper on automatic pruning scheme mapping is accepted at <b> TODAES </b>  </li>
                    <li> 09/2021: Our paper on memory-Economic sparse training is accepted at <b> NeurIPS 2021 </b>  </li>
                    <li> 07/2021: Our paper on NAS and pruning search for real-time super-resolution on mobile is accepted at <b> ICCV 2021 </b>  </li>
                    <li> 02/2021: Our paper NPAS is accepted at <b> CVPR 2021 </b>  </li>
                    <li> 02/2021: Our paper on weight pruning using reweighted optimization methods is accepted at <b> DAC 2021 </b>  </li>
                    
                    <li> <a href="javascript:toggle_vis('news')">Earlier news</a> </li>
                    <div id="news" style="display:none">
                      <li> 06/2020: Our paper on privacy-preserving-oriented DNN pruning is invited at <b> GLSVLSI 2021 </b>  </li>
                      <li> 04/2019: Our paper Chic is accepted at <b> IWQoS 2019 </b>  </li>
                      <li> 11/2018: Our paper on universal approximation property and equivalence is accepted at <b> AAAI 2019 </b>  </li>
                    </div>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experiences</heading>
                  <p>
                  <ul id="exp" style="list-style:none;">
                    <li> <img class="school-logo" src="images/exp_logos/NEU.jpeg">
                      <div class="school-text">Sep 2019 - Present, Northeastern University, <br />Research assistant
                        <br /> Advisor: Prof. Yanzhi Wang
                      </div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/samsung.png">
                      <div class="school-text">May 2022 - Augest 2022, Samsung Research America, <br />Research Intern
                      </div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/llnl.png">
                      <div class="school-text">May 2021 - Augest 2021, Lawrence Livermore National Laboratory, <br />Research Intern
                      </div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/syracuse.png">
                      <div class="school-text">Sep 2017 - May 2019, Syracuse University, <br />Research assistant 
                        <br /> Advisor: Prof. Yanzhi Wang
                      </div>
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Selected Publications</heading> <br>
                <a href="https://scholar.google.com/citations?user=hwTuEX0AAAAJ&hl=en">Google Scholar</a>
                <font color="grey">for all publications</font>. * means equal contribution.
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/sparcl.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/sparcl.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://arxiv.org/abs/2209.09476">
                  <papertitle>SparCL: Sparse Continual Learning on the Edge</papertitle>
                </a>
                <br>
                Zifeng Wang*, <b>Zheng Zhan*</b>, Yifan Gong, Geng Yuan, Wei Niu, Tong Jian,
                Bin Ren, Stratis Ioannidis, Yanzhi Wang, Jennifer Dy<br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br> </em>
                Paper and code will be released soon.
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> SparCL introduces a new continual learning scheme whose training efficiency and effectiveness are achieved 
                  through three key components that encourage sparse network weight connection, replay buffer selection, 
                  and sparse gradient truncation. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/all_in_one.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/all_in_one.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <papertitle>All-in-One: A Highly Representative DNN Pruning Framework for Edge Devices with Dynamic Power Management</papertitle>
                </a>
                <br>
                Yifan Gong*, <b>Zheng Zhan*</b>, Pu Zhao, Yushu Wu, Chao Wu, Caiwen Ding, Weiwen Jiang, Minghai Qin, Yanzhi Wang<br>
                <em>International Conference on Computer-Aided Design (<b>ICCAD</b>), 2022. <br> </em>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> All-in-One, a highly representative pruning framework to work with dynamic power management using DVFS. 
                  The framework can use only one set of model weights and soft masks to represent multiple models of various 
                  pruning ratios. By re-configuring the model to the corresponding pruning ratio for a specific execution frequency 
                  (and voltage), we can keep the difference in speed performance under various execution frequencies as small as possible. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/eccv22.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/eccv22.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2207.12577.pdf">
                  <papertitle>Compiler-Aware Neural Architecture Search for On-Mobile Real-time Super-Resolution</papertitle>
                </a>
                <br>
                Yushu Wu*, Yifan Gong*, Pu Zhao, Yanyu Li, <b>Zheng Zhan</b>, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, Yanzhi Wang<br>
                <em>European Conference on Computer Vision  (<b>ECCV</b>), 2022. <br> </em>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We propose a compiler-aware SR neural architecture search (NAS) framework that conducts depth search 
                  and per-layer width search with adaptive SR blocks. A speed model incorporated with compiler optimizations is leveraged 
                  to predict the inference latency of the SR block with various width configurations for faster convergence. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/todaes22.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/todaes22.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2111.11581.pdf">
                  <papertitle>Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration</papertitle>
                </a>
                <br>
                Yifan Gong*, Geng Yuan*, <b>Zheng Zhan</b>, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, Xulong Tang, Yanzhi Wang<br>
                <em>Transactions on Design Automation of Electronic Systems (<b>TODAES</b>), 2022. <br> </em>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We propose a general, fine-grained structured pruning scheme and corresponding compiler optimizations that 
                  are applicable to any type of DNN layer while achieving high accuracy and hardware inference performance. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/mest.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/mest.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2110.14032">
                  <papertitle>Mest: Accurate and fast memory-economic sparse training framework on the edge</papertitle>
                </a>
                <br>
                Geng Yuan*, Xiaolong Ma*, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, <b>Zheng Zhan</b>, 
                Chaoyang He, Qing Jin, Siyue Wang, Minghai Qin, Bin Ren, Yanzhi Wang, Sijia Liu, Xue Lin<br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2021. <br> </em>
                <font color=red> Spotlight </font> <br>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> The proposed MEST framework consists of enhancements by Elastic Mutation (EM) and Soft Memory Bound (&S) that 
                  ensure superior accuracy at high sparsity ratios. On top of that, we proposes
                  to employ data efficiency for further acceleration of sparse training. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/iccv21.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/iccv21.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2108.08910.pdf">
                  <papertitle>Achieving on-mobile real-time super-resolution with neural architecture and pruning search</papertitle>
                </a>
                <br>
                <b>Zheng Zhan*</b>, Yifan Gong*, Pu Zhao*, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, Xue Lin, Yanzhi Wang<br>
                <em>International Conference on Computer Vision (<b>ICCV</b>), 2021. <br> </em>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We proposes to use neural architecture search and network pruning to find a highly efficient network
                  for image super-resolution. we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) 
                  for implementing 720p resolution with competitive perceptual performance on mobile platforms. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/DAC21.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/DAC21.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2004.05531.pdf">
                  <papertitle>A unified DNN weight compression framework using reweighted optimization methods</papertitle>
                </a>
                <br>
                Tianyun Zhang, Xiaolong Ma, <b>Zheng Zhan</b>, Shanglin Zhou, Caiwen Ding, Makan Fardad, Yanzhi Wang<br>
                <em>Design Automation Conference (<b>DAC</b>), 2021. <br> </em>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We propose a unified DNN weight pruning framework with dynamically updated regularization terms bounded 
                  by the designated constraint, which can generate both non-structured sparsity and different kinds of structured sparsity. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/cvpr21.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/cvpr21.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2012.00596.pdf">
                  <papertitle>NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration</papertitle>
                </a>
                <br>
                Zhengang Li*, Geng Yuan*, Wei Niu*, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan Shen, <b>Zheng Zhan</b>, Zhenglun Kong, Qing Jin, Zhiyu Chen, Sijia Liu, Kaiyuan Yang, Bin Ren, Yanzhi Wang, Xue Lin<br>
                <em>Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>), 2021. <br> </em>
                <font color=red> Oral </font> <br>
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> In this work, we first propose (i) a general category of fine-grained structured pruning applicable to 
                  various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting 
                  different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. 
                  We further propose NPAS, a compiler-aware unified network pruning, and architecture search.  </p>
              </td>
            </tr>

                
            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/iwqos19.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/iwqos19.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.comm.toronto.edu/~liang/publications/IWQoS19.pdf">
                  <papertitle>Chic experience-driven scheduling in machine learning clusters</papertitle>
                </a>
                <br>
                Yifan Gong, Baochun Li, Ben Liang, <b>Zheng Zhan</b><br>
                <em>International Symposium on Quality of Service(<b>IWQoS</b>), 2019. <br> </em>

                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We design an experience-driven approach that learns to manage the cluster directly from experience rather 
                  than using a mathematical model. We propose Chic, a scheduler that is tailored for scheduling machine
                  learning workloads in a cluster by leveraging deep reinforcement learning techniques. </p>
              </td>
            </tr>
                
            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/AAAI19.jpg" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/AAAI19.jpg" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.33015369">
                  <papertitle>Universal approximation property and equivalence of stochastic computing-based neural networks and binary neural networks</papertitle>
                </a>
                <br>
                Yanzhi Wang, <b>Zheng Zhan</b>, Liang Zhao, Jian Tang, Siyue Wang, Jiayu Li, Bo Yuan, Wujie Wen, Xue Lin<br>
                <em>AAAI Conference on Artificial Intelligence(<b>AAAI</b>), 2019. <br> </em>

                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> We prove that the ”ideal” SCNNs and BNNs satisfy the universal approximation property with probability 1 (due to 
                  the stochastic behavior),  we further prove that SCNNs and BNNs exhibit the same energy complexity</p>
              </td>
            </tr>
                
          </table>

            <p align="right">
              Template Credit: <a href="https://jonbarron.info/">Jon Barron</a>
            </p>

          </table> <!--  Intro + Publications -->



        </td>
      </tr>
  </table>
  <!--main text-->

</body>

</html>
